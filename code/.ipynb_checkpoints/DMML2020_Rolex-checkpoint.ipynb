{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚åöÔ∏è DM&ML 2020 - Team Rolex\n",
    "\n",
    "## üñã Authors\n",
    "- Francis Ruckstuhl, 16-821-738\n",
    "- Hanna Birbaum, 16-050-114\n",
    "- Lo√Øc Rouiller-Monay, 16-832-453\n",
    "\n",
    "## üïµÔ∏è Project description\n",
    "\n",
    "Real or Not? NLP with Disaster Tweets: Machine Learning model that can predict which tweets are about a real disaster and which are not. The project topic is based around a Kaggle competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üìù Commits\n",
    "\n",
    "### Best commit:\n",
    "\n",
    "**Commit 2 : 0.818%**\n",
    "- data cleaning : remove unicode literals, urls, link, author, hashtags, rt\n",
    "- feature engineering : num_chars, num_words, avg_words\n",
    "- BOW\n",
    "- LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "\n",
    "### [B.] Previous commits\n",
    "\n",
    "**Commit 1 : 0.808%**\n",
    "- spacy_tokenizer: remove stopwords, punctuation, numbers then lemmatize and lowercase\n",
    "- TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), tokenizer=spacy_tokenizer)\n",
    "- LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "\n",
    "**Commit 2 : 0.818%**\n",
    "- data cleaning : remove unicode literals, urls, link, author, hashtags, rt\n",
    "- feature engineering : num_chars, num_words, avg_words\n",
    "- BOW\n",
    "- LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "\n",
    "**Commit 3 : 0.809%**\n",
    "- data cleaning : remove unicode literals, urls, link, author, hashtags, rt, punctuations, lowercase\n",
    "- feature engineering : num_chars, num_words, avg_words, num_hashtags\n",
    "- BOW\n",
    "- LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "\n",
    "**Commit 4 : 0.801%**\n",
    "- data cleaning : remove unicde literals, urls, link, author, hashtags, rt, punctuations, lowercase, lemmatize, stemming\n",
    "- model_dbow = Doc2Vec(dm=0, vector_size=30, negative=6, hs=0, min_count=1, sample=0, workers=cores, epoch=300)\n",
    "- Word2Vec\n",
    "- LogisticRegression(max_iter=1000, solver='lbfgs')\n",
    "\n",
    "**Commit 5 : 0.812%**\n",
    "- Same as Commit 4 but without stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###¬†[C.] Progression of accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /!\\ You have to run Chapter 1. \"libraries‚Äú first before being able to plot the progression of accuracies\n",
    "accuracy_progression = pd.read_csv('../documents/accuracy_progression.csv', sep=';')\n",
    "sns.lineplot(x=accuracy_progression.commit_number, y=accuracy_progression.accuracy, linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "import spacy\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "# load English language model of spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "import string\n",
    "from spellchecker import SpellChecker\n",
    "import pycountry\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "# Yet to discuss whether this will be used or not\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìÇ 2. Download data\n",
    "\n",
    "\n",
    "## Files\n",
    "- train.csv - the training set\n",
    "- test.csv - the test set\n",
    "- sample_submission.csv - a sample submission file in the correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/training_data.csv')\n",
    "test = pd.read_csv('../data/test_data.csv')\n",
    "sample_submission = pd.read_csv('../data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9972</td>\n",
       "      <td>tsunami</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Crptotech tsunami and banks.\\r\\n http://t.co/K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9865</td>\n",
       "      <td>traumatised</td>\n",
       "      <td>Portsmouth, UK</td>\n",
       "      <td>I'm that traumatised that I can't even spell p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1937</td>\n",
       "      <td>burning%20buildings</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@foxnewsvideo @AIIAmericanGirI @ANHQDC So ... ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3560</td>\n",
       "      <td>desolate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Me watching Law &amp;amp; Order (IB: @sauldale305)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2731</td>\n",
       "      <td>crushed</td>\n",
       "      <td>bahstun/porta reeko</td>\n",
       "      <td>Papi absolutely crushed that ball</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id              keyword             location  \\\n",
       "0  9972              tsunami                  NaN   \n",
       "1  9865          traumatised       Portsmouth, UK   \n",
       "2  1937  burning%20buildings                  NaN   \n",
       "3  3560             desolate                  NaN   \n",
       "4  2731              crushed  bahstun/porta reeko   \n",
       "\n",
       "                                                text  \n",
       "0  Crptotech tsunami and banks.\\r\\n http://t.co/K...  \n",
       "1  I'm that traumatised that I can't even spell p...  \n",
       "2  @foxnewsvideo @AIIAmericanGirI @ANHQDC So ... ...  \n",
       "3  Me watching Law &amp; Order (IB: @sauldale305)...  \n",
       "4                  Papi absolutely crushed that ball  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "- id - a unique identifier for each tweet\n",
    "- text - the text of the tweet\n",
    "- location - the location the tweet was sent from (may be blank)\n",
    "- keyword - a particular keyword from the tweet (may be blank)\n",
    "- target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6471 entries, 0 to 6470\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        6471 non-null   int64 \n",
      " 1   keyword   6416 non-null   object\n",
      " 2   location  4330 non-null   object\n",
      " 3   text      6471 non-null   object\n",
      " 4   target    6471 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 252.9+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['num_hashtags'] = train['text'].apply(lambda x: x.count('#'))\n",
    "test['num_hashtags'] = test['text'].apply(lambda x: x.count('#'))\n",
    "\n",
    "train[\"avg_word_length\"] = train['text'].apply(lambda x: np.sum([len(w) for w in x.split()]) / len(x.split()))\n",
    "test[\"avg_word_length\"] = test['text'].apply(lambda x: np.sum([len(w) for w in x.split()]) / len(x.split()))\n",
    "\n",
    "train[\"num_char\"] = train[\"text\"].apply(len)\n",
    "test[\"num_char\"] = test[\"text\"].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disaster Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regex for countries that require cleaning:\n",
    "\n",
    "# United States:\n",
    "usa_regex = re.compile(r\"\"\"(?i)Alabama|\\bAL\\b|Alaska|\\bAK\\b|Arizona|\\bAZ\\b|Arkansas|\\bAR\\b|California|\\bCA\\b|Colorado|\\bCO\\b|\n",
    "                Connecticut|\\bCT\\b|Delaware|\\bDE\\b|Florida|\\bFL\\b|Georgia|\\bGA\\b|Hawaii|\\bHI\\b|Idaho|\\bID\\b|Illinois|\\bIL\\b|\n",
    "                Indiana|\\bIN\\b|Iowa\\bIA\\b|Kansas|\\bKS\\b|Kentucky|\\bKY\\b|Louisiana|\\bLA\\b|Maine|\\bME\\b|Maryland|\\bMD\\b|Massachusetts|\n",
    "                \\bMA\\b|Michigan|\\bMI\\b|Minnesota|\\bMN\\b|Mississippi|\\bMS\\b|Missouri|\\bMO\\b|Montana|\\bMT\\b|Nebraska|\\bNE\\b|Nevada|\n",
    "                \\bNV\\b|New\\sHampshire|\\bNH\\b|New\\sJersey|\\bNJ\\b|New Mexico|\\bNM\\b|New\\sYork|\\bNY\\b|\\bNYC\\b|North\\sCarolina|\\bNC\\b|\n",
    "                North\\sDakota|\\bND\\b|Ohio|\\bOH\\b|Oklahoma|\\bOK\\b|Oregon|\\bOR\\b|Pennsylvania|\\bPA\\b|Rhode\\sIsland|\\bRI\\b|South\\sCarolina|\n",
    "                \\bSC\\b|South\\sDakota|\\bSD\\b|Tennessee|\\bTN\\b|Texas|\\bTX\\b|Utah|\\bUT\\b|Vermont|\\bVT\\b|Virginia|\\bVA\\b|Washington|\\bWA\\b|\n",
    "                West\\sVirginia|\\bWV\\b|Wisconsin|\\bWI\\b|Wyoming|\\bWY\\b|\\bUSA\\b|San\\sFrancisco|Los\\sAngeles|Seattle|Chicago|\n",
    "                Atlanta\"\"\", re.VERBOSE)\n",
    "\n",
    "# United Kingdom:\n",
    "uk_regex = re.compile(r\"\"\"(?i)UK|London|England|Scotland|Wales|Birmingham|Glasgow|Liverpool|Bristol|Manchester|\n",
    "                      Sheffield|Leeds|Edinburgh|Leicester|Coventry|Bradford|Cardiff|Belfast|Oxford|Plymouth|Aberdeen\"\"\", re.VERBOSE)\n",
    "\n",
    "# Canada:\n",
    "ca_regex = re.compile(r\"\"\"(?i)Canada|Ontario|Quebec|Nova\\sScotia|New Brunswick|Manitoba|British\\sColumbia|Prince\\sEdward\\sIsland|\n",
    "                      Saskatchewan|Alberta|Newfoundland|Labrator|Toronto|Ottawa|Vancouver|Calgary\"\"\", re.VERBOSE)\n",
    "\n",
    "# Australia:\n",
    "au_regex = re.compile(r\"\"\"(?i)australia|Brisbane|Melbourne|Sydney|Perth|Adelaide|Capital\\sTerritory|Canberra|Hobart|\n",
    "                      Darwin|Gold\\sCoast|Queensland|Victoria|Tasmania\"\"\", re.VERBOSE)\n",
    "\n",
    "# India:\n",
    "in_regex = re.compile(r\"\"\"(?i)mumbai|Maharashtra|Delhi|Kolkata|West\\sBengal|Chennai|Tamil\\sNadu|Hyderabad|Bangalore|\n",
    "                      Ahmedabad|Surat|Jaipur|Kanpur|Nagpur|Gujarat|Uttar\\sPradesh\"\"\", re.VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the rows and check if any of the locations matches one of our regexes\n",
    "# If so, the entire value will be replaced by a unified name:\n",
    "\n",
    "for index, row in train.iterrows():\n",
    "\n",
    "  # For any location in the United States:\n",
    "    if re.search(usa_regex, str(train.loc[index, \"location\"])):\n",
    "        train.loc[index, \"country\"] = \"United States\"\n",
    "\n",
    "  # For any location in the United Kingdom:\n",
    "    elif re.search(uk_regex, str(train.loc[index, \"location\"])):\n",
    "        train.loc[index, \"country\"] = \"United Kingdom\"\n",
    "\n",
    "  # For any location in Canada:\n",
    "    elif re.search(ca_regex, str(train.loc[index, \"location\"])):\n",
    "        train.loc[index, \"country\"] = \"Canada\"\n",
    "  \n",
    "  # For any location in Australia:\n",
    "    elif re.search(au_regex, str(train.loc[index, \"location\"])):\n",
    "        train.loc[index, \"country\"] = \"Australia\"\n",
    "  \n",
    "  # For any location in the India:\n",
    "    elif re.search(in_regex, str(train.loc[index, \"location\"])):\n",
    "        train.loc[index, \"country\"] = \"India\"\n",
    "        \n",
    "        \n",
    "for index, row in test.iterrows():\n",
    "\n",
    "  # For any location in the United States:\n",
    "    if re.search(usa_regex, str(test.loc[index, \"location\"])):\n",
    "        test.loc[index, \"country\"] = \"United States\"\n",
    "\n",
    "  # For any location in the United Kingdom:\n",
    "    elif re.search(uk_regex, str(test.loc[index, \"location\"])):\n",
    "        test.loc[index, \"country\"] = \"United Kingdom\"\n",
    "\n",
    "  # For any location in Canada:\n",
    "    elif re.search(ca_regex, str(test.loc[index, \"location\"])):\n",
    "        test.loc[index, \"country\"] = \"Canada\"\n",
    "  \n",
    "  # For any location in Australia:\n",
    "    elif re.search(au_regex, str(test.loc[index, \"location\"])):\n",
    "        test.loc[index, \"country\"] = \"Australia\"\n",
    "  \n",
    "  # For any location in the India:\n",
    "    elif re.search(in_regex, str(test.loc[index, \"location\"])):\n",
    "        test.loc[index, \"country\"] = \"India\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßπ 4. Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove '%20' from keyword feature\n",
    "train.keyword = train.keyword.apply(lambda lex: str(lex).replace('%20', ' '))\n",
    "test.keyword = test.keyword.apply(lambda ro: str(ro).replace('%20', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ† [D.] 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.country = train.country.fillna('nocountry')\n",
    "train.keyword = train.keyword.fillna('nokeyword')\n",
    "train.location = train.location.fillna('nolocation')\n",
    "train.text = train.text.fillna('notext')\n",
    "\n",
    "test.country = test.country.fillna('nocountry')\n",
    "test.keyword = test.keyword.fillna('nokeyword')\n",
    "test.location = test.location.fillna('nolocation')\n",
    "test.text = test.text.fillna('notext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.country = train.country.astype(str)\n",
    "train.keyword = train.keyword.astype(str)\n",
    "train.location = train.location.astype(str)\n",
    "train.text = train.text.astype(str)\n",
    "\n",
    "test.country = test.country.astype(str)\n",
    "test.keyword = test.keyword.astype(str)\n",
    "test.location = test.location.astype(str)\n",
    "test.text = test.text.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we caanot use multiple columns in the models except with the bow, we will try to make a column regrouping all the colmns\n",
    "train['full'] = train['country'].str.cat(train[['location', 'keyword', 'text']], sep=\" \")\n",
    "test['full'] = test['country'].str.cat(test[['location', 'keyword', 'text']], sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove '%20' from keyword feature\n",
    "#train.full = train.full.apply(lambda lex: str(lex).replace('nan', ''))\n",
    "#test.full = test.full.apply(lambda ro: str(ro).replace('nan', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crptotech tsunami and banks.\r\n",
      " http://t.co/KHzTeVeDja #Banking #tech #bitcoing #blockchain\n",
      "I'm that traumatised that I can't even spell properly! Excuse the typos!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    None\n",
       "1    None\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.text.head(2).apply(lambda x: print(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nocountry nolocation tsunami Crptotech tsunami and banks.\r\n",
      " http://t.co/KHzTeVeDja #Banking #tech #bitcoing #blockchain\n",
      "United Kingdom Portsmouth, UK traumatised I'm that traumatised that I can't even spell properly! Excuse the typos!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    None\n",
       "1    None\n",
       "Name: full, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.full.head(2).apply(lambda x: print(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.keyword.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è 6. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer function for preprocessing\n",
    "def spacy_tokenizer2(text):\n",
    "\n",
    "    # Define stopwords, punctuation, rolex and numbers\n",
    "    stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "    punctuations = string.punctuation\n",
    "    \n",
    "     # remove unicode literals\n",
    "    temp = text.encode('ascii',errors='ignore').decode('ascii')\n",
    "    \n",
    "    # remove &amp\n",
    "    temp = temp.replace('&amp;', '&')\n",
    "    \n",
    "    # remove urls\n",
    "    temp = re.sub(r\"http\\S+\", \"\", temp)\n",
    "    \n",
    "    # remove html\n",
    "    temp = re.sub(r'<.*?>', \"\", temp)\n",
    "    \n",
    "    # remove hashtags\n",
    "    temp = re.sub(r'#', \"\", temp)\n",
    "    \n",
    "    # remove accounts\n",
    "    temp = re.sub(r\"@\\S+\", \"\", temp)\n",
    "\n",
    "    # Create spacy object\n",
    "    mytokens = sp(temp)\n",
    "\n",
    "    #Lemmatize each token and convert each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "\n",
    "    # Remove stop words and punctuation\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "\n",
    "    # Return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nocountry', 'nolocation', 'burn', 'building', '...', 'rioter', 'looter', 'burn', 'building', 'white', 'live', 'matter']\n"
     ]
    }
   ],
   "source": [
    "print(spacy_tokenizer2(test.full[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW bang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Using tokenizer \n",
    "count = CountVectorizer(ngram_range=(1,6), min_df=3, tokenizer=spacy_tokenizer2)\n",
    "bow = count.fit_transform(train.full)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = count.get_feature_names()\n",
    "\n",
    "# Show as a dataframe\n",
    "processed_train = pd.DataFrame(\n",
    "    bow.todense(), \n",
    "    columns=feature_names\n",
    "    )\n",
    "\n",
    "processed_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full = pd.concat([train[['num_char', 'num_hashtags', 'avg_word_length']], processed_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features\n",
    "X = train_full # the features we want to analyze\n",
    "y = train['target'] # the labels, or answers, we want to test against\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=707)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()  # doctest: +SKIP\n",
    "# Don't cheat - fit only on training data\n",
    "scaler.fit(X_train)  # doctest: +SKIP\n",
    "X_train = scaler.transform(X_train)  # doctest: +SKIP\n",
    "# apply same transformation to test data\n",
    "X_test = scaler.transform(X_test)  # doctest: +SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Define PCA\n",
    "pca = PCA(n_components=3000)\n",
    "\n",
    "# Example on X_train_vec\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "print('Shape after PCA: ', X_train_vec_pca.shape)\n",
    "print('Number of components: ', pca.n_components_)\n",
    "print('Explained variance ratio: ', sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classifier\n",
    "classifier = LogisticRegressionCV(solver='liblinear', max_iter=2000, cv=5, class_weight='balanced', n_jobs=-1)\n",
    "#classifier = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit model on training set\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = classifier.predict(X_test)\n",
    "train_pred = classifier.predict(X_train)\n",
    "\n",
    "# Evaluate model\n",
    "print(round(accuracy_score(train_pred, y_train), 4))\n",
    "print(round(accuracy_score(y_test, y_pred), 4))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features\n",
    "X = train['text'] # the features we want to analyze\n",
    "y = train['target'] # the labels, or answers, we want to test against\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=707)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Define vectorizer\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 1), tokenizer=spacy_tokenizer)\n",
    "\n",
    "# Define classifier\n",
    "classifier = LogisticRegressionCV(solver='lbfgs', max_iter=1000, cv=5)\n",
    "\n",
    "# Create pipeline\n",
    "pipe = Pipeline([('vectorizer', tfidf),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "# Fit model on training set\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "print(round(accuracy_score(y_test, y_pred), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perhaps a random forest? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe try a Random Forest? (- Hanna)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define vectorizer\n",
    "tfidf_vector = TfidfVectorizer(tokenizer=spacy_tokenizer) \n",
    "\n",
    "# Define classifier\n",
    "classifier = RandomForestClassifier()\n",
    "\n",
    "# Create pipeline\n",
    "pipe = Pipeline([('vectorizer', tfidf_vector),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "# Fit model on training set\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigbang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features\n",
    "X = train['full'] # the features we want to analyze\n",
    "y = train['target'] # the labels, or answers, we want to test against\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=707)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5176, 51904)\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Define vectorizer - use above cleaning function\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, ngram_range=(1,2), tokenizer=spacy_tokenizer2)\n",
    "\n",
    "# Fit and transform X_train and X_test\n",
    "X_train_vec = tfidf.fit_transform(X_train).toarray()\n",
    "X_test_vec = tfidf.transform(X_test).toarray()\n",
    "print(X_train_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Define Model\n",
    "pipe = Pipeline([\n",
    "                 ('scaler', StandardScaler()),\n",
    "                 ('logistic reg', LogisticRegressionCV(solver='sag', max_iter=2000, cv=5, class_weight='balanced', n_jobs=-1))\n",
    "                 ])\n",
    "# Fit model\n",
    "pipe.fit(X_train_vec, y_train)\n",
    "y_pred = pipe.predict(X_test_vec)\n",
    "\n",
    "print('Train Accuracy: ', round(pipe.score(X_train_vec, y_train), 4))\n",
    "print('Test Accuracy: ', round(pipe.score(X_test_vec, y_test), 4))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Define Model\n",
    "pipe = Pipeline([\n",
    "                 ('scaler', StandardScaler()),\n",
    "                 ('logistic reg', LogisticRegressionCV(solver='lbfgs', max_iter=5000, cv=5))\n",
    "                 ])\n",
    "# Fit model\n",
    "pipe.fit(X_train_vec, y_train)\n",
    "print('Train Accuracy: ', round(pipe.score(X_train_vec, y_train), 4))\n",
    "print('Test Accuracy: ', round(pipe.score(X_test_vec, y_test), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Define Model\n",
    "pipe = Pipeline([\n",
    "                 ('logistic reg', LogisticRegressionCV(solver='lbfgs', max_iter=5000, cv=7))\n",
    "                 ])\n",
    "# Fit model\n",
    "pipe.fit(X_train_vec_pca, y_train)\n",
    "print('Train Accuracy: ', round(pipe.score(X_train_vec_pca, y_train), 4))\n",
    "print('Test Accuracy: ', round(pipe.score(X_test_vec_pca, y_test), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Define Model\n",
    "pipe = Pipeline([\n",
    "                 ('logistic reg', LogisticRegressionCV(solver='lbfgs', max_iter=5000, cv=8))\n",
    "                 ])\n",
    "# Fit model\n",
    "pipe.fit(X_train_vec_pca, y_train)\n",
    "print('Train Accuracy: ', round(pipe.score(X_train_vec_pca, y_train), 4))\n",
    "print('Test Accuracy: ', round(pipe.score(X_test_vec_pca, y_test), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Define Model\n",
    "pipe = Pipeline([\n",
    "                 ('logistic reg', LogisticRegressionCV(solver='lbfgs', max_iter=5000, cv=10))\n",
    "                 ])\n",
    "# Fit model\n",
    "pipe.fit(X_train_vec_pca, y_train)\n",
    "print('Train Accuracy: ', round(pipe.score(X_train_vec_pca, y_train), 4))\n",
    "print('Test Accuracy: ', round(pipe.score(X_test_vec_pca, y_test), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Define Model\n",
    "pipe = Pipeline([\n",
    "                 ('logistic reg', LogisticRegressionCV(solver='lbfgs', max_iter=5000, cv=12))\n",
    "                 ])\n",
    "# Fit model\n",
    "pipe.fit(X_train_vec_pca, y_train)\n",
    "print('Train Accuracy: ', round(pipe.score(X_train_vec_pca, y_train), 4))\n",
    "print('Test Accuracy: ', round(pipe.score(X_test_vec_pca, y_test), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Define Model\n",
    "pipe = Pipeline([\n",
    "                 ('logistic reg', LogisticRegressionCV(solver='lbfgs', max_iter=6000, cv=15))\n",
    "                 ])\n",
    "# Fit model\n",
    "pipe.fit(X_train_vec_pca, y_train)\n",
    "print('Train Accuracy: ', round(pipe.score(X_train_vec_pca, y_train), 4))\n",
    "print('Test Accuracy: ', round(pipe.score(X_test_vec_pca, y_test), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Define Model\n",
    "pipe = Pipeline([\n",
    "                 ('logistic reg', LogisticRegressionCV(solver='lbfgs', max_iter=10000, cv=20))\n",
    "                 ])\n",
    "# Fit model\n",
    "pipe.fit(X_train_vec_pca, y_train)\n",
    "print('Train Accuracy: ', round(pipe.score(X_train_vec_pca, y_train), 4))\n",
    "print('Test Accuracy: ', round(pipe.score(X_test_vec_pca, y_test), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Define Model\n",
    "pipe = Pipeline([\n",
    "                 ('scaler', scaler),\n",
    "                 ('pca', pca),\n",
    "                 ('logistic reg', LogisticRegressionCV(solver='lbfgs', max_iter=5000, cv=5)),\n",
    "                 ])\n",
    "# Fit model\n",
    "pipe.fit(X_train_vec, y_train)\n",
    "print('Train Accuracy: ', round(pipe.score(X_train_vec, y_train), 4))\n",
    "print('Test Accuracy: ', round(pipe.score(X_test_vec, y_test), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Define Model\n",
    "pipe = Pipeline([\n",
    "                 ('pca', pca),\n",
    "                 ('knn', KNeighborsClassifier(20)),\n",
    "                 ])\n",
    "# Fit model\n",
    "pipe.fit(X_train_vec, y_train)\n",
    "print('Train Accuracy: ', round(pipe.score(X_train_vec, y_train), 4))\n",
    "print('Test Accuracy: ', round(pipe.score(X_test_vec, y_test), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Define Model\n",
    "pipe = Pipeline([\n",
    "                 ('scaler', scaler),\n",
    "                 ('pca', pca),\n",
    "                 ('knn', KNeighborsClassifier(20)),\n",
    "                 ])\n",
    "# Fit model\n",
    "pipe.fit(X_train_vec, y_train)\n",
    "print('Train Accuracy: ', round(pipe.score(X_train_vec, y_train), 4))\n",
    "print('Test Accuracy: ', round(pipe.score(X_test_vec, y_test), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tree_para = {'criterion':['gini','entropy'],'max_depth':[4,8,12,15,30,70,90,120,150]}\n",
    "\n",
    "# Define Model\n",
    "pipe = Pipeline([\n",
    "                 ('pca', pca),\n",
    "                 ('gscv', GridSearchCV(DecisionTreeClassifier(), tree_para, cv=5)),\n",
    "                 ])\n",
    "# Fit model\n",
    "\n",
    "pipe.fit(X_train_vec, y_train)\n",
    "print('Train Accuracy: ', round(pipe.score(X_train_vec, y_train), 4))\n",
    "print('Test Accuracy: ', round(pipe.score(X_test_vec, y_test), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using Doc2Vec and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tagged = train.apply(lambda r: TaggedDocument(words=spacy_tokenizer(r['text']), tags=[r.target]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split - same split as before\n",
    "train_tagged, test_tagged = train_test_split(sample_tagged, test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows to speed up a bit\n",
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Doc2Vec and build vocabulary\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "model_dbow = Doc2Vec(dm=1, vector_size=1000, negative=5, hs=0, min_count=1, sample=0, workers=cores, epoch=500)\n",
    "model_dbow.build_vocab([x for x in train_tagged.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train distributed Bag of Word model\n",
    "model_dbow.train(train_tagged, total_examples=model_dbow.corpus_count, epochs=model_dbow.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select X and y\n",
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=300)) for doc in sents])\n",
    "    return targets, regressors\n",
    "\n",
    "y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dbow, test_tagged)\n",
    "\n",
    "# Each document (i.e. complaint) is now a vector in the space of 30 dimentions.\n",
    "# Similar complaints should have similar vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model on training set - same algorithm as before\n",
    "logreg = LogisticRegressionCV(max_iter=3000, cv=10, solver='lbfgs')\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "print(round(accuracy_score(y_test, y_pred), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using Doc2Vec, more features and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tagged = train.apply(lambda r: TaggedDocument(words=r['text'], tags=[r.target]), axis=1)\n",
    "\n",
    "# Train test split - same split as before\n",
    "train_tagged, test_tagged = train_test_split(sample_tagged, test_size=0.2, random_state=707)\n",
    "\n",
    "# Allows to speed up a bit\n",
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec.load('./doc2vec.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select X and y\n",
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=300)) for doc in sents])\n",
    "    return targets, regressors\n",
    "\n",
    "y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dbow, test_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model on training set - same algorithm as before\n",
    "logreg = LogisticRegressionCV(max_iter=3000, cv=10, solver='lbfgs')\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèÜ 8. Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using default tokenizer \n",
    "count = CountVectorizer(ngram_range=(1,10), stop_words=\"english\", min_df = 5, max_df = 0.8, sublinear_tf=True)\n",
    "bow = count.fit(train.text)\n",
    "bow = count.transform(train.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names\n",
    "feature_names = count.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show as a dataframe\n",
    "processed_train = pd.DataFrame(\n",
    "    bow.todense(), \n",
    "    columns=feature_names\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full = pd.concat([train[['num_char', 'num_words', 'avg_word_length', 'num_hashtags']], processed_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features\n",
    "X = train_full # the features we want to analyze\n",
    "y = train['target'] # the labels, or answers, we want to test against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classifier\n",
    "classifier = LogisticRegressionCV(solver='lbfgs', max_iter=6000, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit model on training set\n",
    "classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_test = count.transform(test.text)\n",
    "# Get feature names\n",
    "feature_names_test = count.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show as a dataframe\n",
    "processed_test = pd.DataFrame(\n",
    "    bow_test.todense(),\n",
    "    columns=feature_names_test\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_full = pd.concat([test[['num_char', 'num_words', 'avg_word_length' , 'num_hashtags']], processed_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "\n",
    "y_pred = classifier.predict(test_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline\n",
    "pipe = Pipeline([('vectorizer', tfidf),\n",
    "                 ('classifier', classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(train.text, train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pipe.predict(test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/training_data_spellchecked.csv')\n",
    "test = pd.read_csv('../data/test_data_spellchecked.csv')\n",
    "\n",
    "train[['location', 'text']] = train[['location', 'text']].astype(str)\n",
    "test['target'] = ''\n",
    "test[['location', 'text']] = test[['location', 'text']].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tagged = train.apply(lambda r: TaggedDocument(words=spacy_tokenizer(r['text']), tags=[r.target]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tagged = test.apply(lambda r: TaggedDocument(words=spacy_tokenizer(r['text']), tags=[r.target]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows to speed up a bit\n",
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Doc2Vec and build vocabulary\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "model_dbow = Doc2Vec(dm=0, vector_size=30, negative=6, hs=0, min_count=1, sample=0, workers=cores, epoch=300)\n",
    "model_dbow.build_vocab([x for x in train_tagged.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train distributed Bag of Word model\n",
    "model_dbow.train(train_tagged, total_examples=model_dbow.corpus_count, epochs=model_dbow.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select X and y\n",
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=300)) for doc in sents])\n",
    "    return targets, regressors\n",
    "\n",
    "y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dbow, test_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegressionCV(max_iter=1000, solver='lbfgs', cv=3)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Define vectorizer - use above cleaning function\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, ngram_range=(1,2), tokenizer=spacy_tokenizer2)\n",
    "\n",
    "# Fit and transform X_train and X_test\n",
    "X_train_vec = tfidf.fit_transform(train.full).toarray()\n",
    "X_test_vec = tfidf.transform(test.full).toarray()\n",
    "print(X_train_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Define PCA\n",
    "pca = PCA(n_components=0.8)\n",
    "\n",
    "# Example on X_train_vec\n",
    "X_train_vec_pca = pca.fit_transform(X_train_vec)\n",
    "print('Shape after PCA: ', X_train_vec_pca.shape)\n",
    "print('Number of components: ', pca.n_components_)\n",
    "print('Explained variance ratio: ', sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_test_vec_pca = pca.transform(X_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Define Model\n",
    "pipe = Pipeline([\n",
    "                 ('scaler', StandardScaler()),\n",
    "                 ('logistic reg', LogisticRegressionCV(solver='sag', max_iter=2000, cv=5, class_weight='balanced', n_jobs=-1)),\n",
    "                 ])\n",
    "# Fit model\n",
    "pipe.fit(X_train_vec, train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train Accuracy: ', round(pipe.score(X_train_vec, train.target), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = pipe.predict(X_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.target = y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv('submission-025.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
