{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚åöÔ∏è DM&ML 2020 - Team Rolex\n",
    "\n",
    "## üñã Authors\n",
    "- Francis Ruckstuhl, 16-821-738\n",
    "- Hanna Birbaum, 16-050-114\n",
    "- Lo√Øc Rouiller-Monay, 16-832-453\n",
    "\n",
    "## üïµÔ∏è Project description\n",
    "\n",
    "Real or Not? NLP with Disaster Tweets: Machine Learning model that can predict which tweets are about a real disaster and which are not. The project topic is based around a Kaggle competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üìù Commits\n",
    "\n",
    "### Best commit:\n",
    "\n",
    "**Commit 2 : 0.818%**\n",
    "- data cleaning : remove unicode literals, urls, link, author, hashtags, rt\n",
    "- feature engineering : num_chars, num_words, avg_words\n",
    "- BOW\n",
    "- LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "\n",
    "### [B.] Previous commits\n",
    "\n",
    "**Commit 1 : 0.808%**\n",
    "- spacy_tokenizer: remove stopwords, punctuation, numbers then lemmatize and lowercase\n",
    "- TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), tokenizer=spacy_tokenizer)\n",
    "- LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "\n",
    "**Commit 2 : 0.818%**\n",
    "- data cleaning : remove unicode literals, urls, link, author, hashtags, rt\n",
    "- feature engineering : num_chars, num_words, avg_words\n",
    "- BOW\n",
    "- LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "\n",
    "**Commit 3 : 0.809%**\n",
    "- data cleaning : remove unicode literals, urls, link, author, hashtags, rt, punctuations, lowercase\n",
    "- feature engineering : num_chars, num_words, avg_words, num_hashtags\n",
    "- BOW\n",
    "- LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "\n",
    "**Commit 4 : 0.801%**\n",
    "- data cleaning : remove unicde literals, urls, link, author, hashtags, rt, punctuations, lowercase, lemmatize, stemming\n",
    "- model_dbow = Doc2Vec(dm=0, vector_size=30, negative=6, hs=0, min_count=1, sample=0, workers=cores, epoch=300)\n",
    "- Word2Vec\n",
    "- LogisticRegression(max_iter=1000, solver='lbfgs')\n",
    "\n",
    "**Commit 5 : 0.812%**\n",
    "- Same as Commit 4 but without stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###¬†[C.] Progression of accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /!\\ You have to run Chapter 1. \"libraries‚Äú first before being able to plot the progression of accuracies\n",
    "accuracy_progression = pd.read_csv('../documents/accuracy_progression.csv', sep=';')\n",
    "sns.lineplot(x=accuracy_progression.commit_number, y=accuracy_progression.accuracy, linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "import spacy\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "# load English language model of spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "import string\n",
    "from spellchecker import SpellChecker\n",
    "import pycountry\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yet to discuss whether this will be used or not\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìÇ 2. Download data\n",
    "\n",
    "\n",
    "## Files\n",
    "- train.csv - the training set\n",
    "- test.csv - the test set\n",
    "- sample_submission.csv - a sample submission file in the correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/training_data.csv')\n",
    "test = pd.read_csv('../data/test_data.csv')\n",
    "sample_submission = pd.read_csv('../data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "- id - a unique identifier for each tweet\n",
    "- text - the text of the tweet\n",
    "- location - the location the tweet was sent from (may be blank)\n",
    "- keyword - a particular keyword from the tweet (may be blank)\n",
    "- target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¨ 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [A.] What is the baserate of the problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_rate = train.target.value_counts().max()/len(train)\n",
    "print(f'\\nThe base rate is {base_rate}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##¬†Target class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"target\", kind=\"count\", data=train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It misses value in two features : keyword and location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing value in \"keyword\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.keyword.isnull().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing value in \"location\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.location.isnull().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"num_char\"] = train[\"text\"].apply(len)\n",
    "test[\"num_char\"] = test[\"text\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='target', y='num_char', data=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Findings\n",
    "Tweets about real disaster seems to be lengthier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"num_words\"] = train[\"text\"].apply(lambda x: len(x.split()))\n",
    "test[\"num_words\"] = test[\"text\"].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='target', y='num_words', data=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Findings\n",
    "Tweets about real disaster do not seem to have more words. Maybe it'll help to take this into consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"avg_word_length\"] = train['text'].apply(lambda x: np.sum([len(w) for w in x.split()]) / len(x.split()))\n",
    "test[\"avg_word_length\"] = test['text'].apply(lambda x: np.sum([len(w) for w in x.split()]) / len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='target', y='avg_word_length', data=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Findings\n",
    "Tweets about real disaster seems to have lengtier average word length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of hashtags in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['num_hashtags'] = train['text'].apply(lambda x: x.count('#'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='target', y='num_hashtags', data=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DISCUSS WITH TEAMMATES ###\n",
    "# Replace NaN values with \"Unknown\"? (NaNs need to be replace for label encoding)\n",
    "train[\"keyword\"] = train[\"keyword\"].fillna(\"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding for keywords\n",
    "label = LabelEncoder()\n",
    "keyword_label = pd.Series(label.fit_transform(train[\"keyword\"]), name=\"keyword_code\")\n",
    "keyword_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perhaps display the most frequent keywords? \n",
    "train[\"keyword\"].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disaster Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where do most disasters occur / where do disaster tweets come from? \n",
    "# Potential problem to take care of: USA and United States are separate; Different US States are also separate;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many different locations are there?\n",
    "train[\"location\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regex for countries that require cleaning:\n",
    "\n",
    "# United States:\n",
    "usa_regex = re.compile(r\"\"\"(?i)Alabama|\\bAL\\b|Alaska|\\bAK\\b|Arizona|\\bAZ\\b|Arkansas|\\bAR\\b|California|\\bCA\\b|Colorado|\\bCO\\b|\n",
    "                Connecticut|\\bCT\\b|Delaware|\\bDE\\b|Florida|\\bFL\\b|Georgia|\\bGA\\b|Hawaii|\\bHI\\b|Idaho|\\bID\\b|Illinois|\\bIL\\b|\n",
    "                Indiana|\\bIN\\b|Iowa\\bIA\\b|Kansas|\\bKS\\b|Kentucky|\\bKY\\b|Louisiana|\\bLA\\b|Maine|\\bME\\b|Maryland|\\bMD\\b|Massachusetts|\n",
    "                \\bMA\\b|Michigan|\\bMI\\b|Minnesota|\\bMN\\b|Mississippi|\\bMS\\b|Missouri|\\bMO\\b|Montana|\\bMT\\b|Nebraska|\\bNE\\b|Nevada|\n",
    "                \\bNV\\b|New\\sHampshire|\\bNH\\b|New\\sJersey|\\bNJ\\b|New Mexico|\\bNM\\b|New\\sYork|\\bNY\\b|\\bNYC\\b|North\\sCarolina|\\bNC\\b|\n",
    "                North\\sDakota|\\bND\\b|Ohio|\\bOH\\b|Oklahoma|\\bOK\\b|Oregon|\\bOR\\b|Pennsylvania|\\bPA\\b|Rhode\\sIsland|\\bRI\\b|South\\sCarolina|\n",
    "                \\bSC\\b|South\\sDakota|\\bSD\\b|Tennessee|\\bTN\\b|Texas|\\bTX\\b|Utah|\\bUT\\b|Vermont|\\bVT\\b|Virginia|\\bVA\\b|Washington|\\bWA\\b|\n",
    "                West\\sVirginia|\\bWV\\b|Wisconsin|\\bWI\\b|Wyoming|\\bWY\\b|\\bUSA\\b|San\\sFrancisco|Los\\sAngeles|Seattle|Chicago|\n",
    "                Atlanta\"\"\", re.VERBOSE)\n",
    "\n",
    "# United Kingdom:\n",
    "uk_regex = re.compile(r\"\"\"(?i)UK|London|England|Scotland|Wales|Birmingham|Glasgow|Liverpool|Bristol|Manchester|\n",
    "                      Sheffield|Leeds|Edinburgh|Leicester|Coventry|Bradford|Cardiff|Belfast|Oxford|Plymouth|Aberdeen\"\"\", re.VERBOSE)\n",
    "\n",
    "# Canada:\n",
    "ca_regex = re.compile(r\"\"\"(?i)Canada|Ontario|Quebec|Nova\\sScotia|New Brunswick|Manitoba|British\\sColumbia|Prince\\sEdward\\sIsland|\n",
    "                      Saskatchewan|Alberta|Newfoundland|Labrator|Toronto|Ottawa|Vancouver|Calgary\"\"\", re.VERBOSE)\n",
    "\n",
    "# Australia:\n",
    "au_regex = re.compile(r\"\"\"(?i)australia|Brisbane|Melbourne|Sydney|Perth|Adelaide|Capital\\sTerritory|Canberra|Hobart|\n",
    "                      Darwin|Gold\\sCoast|Queensland|Victoria|Tasmania\"\"\", re.VERBOSE)\n",
    "\n",
    "# India:\n",
    "in_regex = re.compile(r\"\"\"(?i)mumbai|Maharashtra|Delhi|Kolkata|West\\sBengal|Chennai|Tamil\\sNadu|Hyderabad|Bangalore|\n",
    "                      Ahmedabad|Surat|Jaipur|Kanpur|Nagpur|Gujarat|Uttar\\sPradesh\"\"\", re.VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the rows and check if any of the locations matches one of our regexes\n",
    "# If so, the entire value will be replaced by a unified name:\n",
    "\n",
    "for index, row in train.iterrows():\n",
    "\n",
    "  # For any location in the United States:\n",
    "  if re.search(usa_regex, str(train.loc[index, \"location\"])):\n",
    "    train.loc[index, \"location\"] = \"United States\"\n",
    "\n",
    "  # For any location in the United Kingdom:\n",
    "  elif re.search(uk_regex, str(train.loc[index, \"location\"])):\n",
    "    train.loc[index, \"location\"] = \"United Kingdom\"\n",
    "\n",
    "  # For any location in Canada:\n",
    "  elif re.search(ca_regex, str(train.loc[index, \"location\"])):\n",
    "    train.loc[index, \"location\"] = \"Canada\"\n",
    "  \n",
    "  # For any location in Australia:\n",
    "  elif re.search(au_regex, str(train.loc[index, \"location\"])):\n",
    "    train.loc[index, \"location\"] = \"Australia\"\n",
    "  \n",
    "  # For any location in the India:\n",
    "  elif re.search(in_regex, str(train.loc[index, \"location\"])):\n",
    "    train.loc[index, \"location\"] = \"India\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the top 5 countries:\n",
    "countries = train[\"location\"].value_counts()\n",
    "countries = countries.sort_values(ascending=False).head(5)\n",
    "countries.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DISCUSS WITH TEAMMATES ###\n",
    "# Will this help with data cleaning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßπ 4. Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove '%20' from keyword feature\n",
    "train.keyword = train.keyword.apply(lambda lex: str(lex).replace('%20', ' '))\n",
    "test.keyword = train.keyword.apply(lambda ro: str(ro).replace('%20', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if NaN values in the keyword feature\n",
    "print(train.keyword.isnull().any())\n",
    "print(test.keyword.isnull().any())\n",
    "\n",
    "# There's no null values between the keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pycountry in order to check if a country appears in the location\n",
    "# if yes takes the country, else turn it to NaN\n",
    "# with train and test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.location.apply(lambda x: print(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "punctuations = string.punctuation\n",
    "\n",
    "def separate_punct(text):\n",
    "    temp = []\n",
    "    for char in text:\n",
    "        if char not in punctuations:\n",
    "            temp.append(char)\n",
    "        else:\n",
    "            temp.append(' '+char)\n",
    "    return ''.join(temp)\n",
    "\n",
    "def clean_text(text):\n",
    "    # remove unicode literals\n",
    "    temp = text.encode('ascii',errors='ignore').decode('ascii')\n",
    "    \n",
    "    # remove &amp\n",
    "    temp = temp.replace('&amp;', '')\n",
    "    \n",
    "    # remove urls\n",
    "    temp = re.sub(r\"http\\S+\", \"\", temp)\n",
    "    \n",
    "    # remove html\n",
    "    temp = re.sub(r'<.*?>', \"\", temp)\n",
    "    \n",
    "    # remove hashtags\n",
    "    temp = re.sub(r'#', \"\", temp)\n",
    "\n",
    "    # remove people account with @\n",
    "    temp = re.sub(r'@\\S+', \"\", temp)\n",
    "    \n",
    "    # remove 'RT'\n",
    "    temp = temp.replace('RT', '')\n",
    "    \n",
    "    # remove punctuation\n",
    "    temp = ''.join([ char for char in temp if char not in punctuations ])\n",
    "    \n",
    "    # separate punctuation\n",
    "    # temp = separate_punct(temp)\n",
    "\n",
    "    # remove \".\"\n",
    "    #temp.replace('.','')\n",
    "    \n",
    "    # lowercase\n",
    "    temp = temp.lower()\n",
    "    \n",
    "    # spell checking\n",
    "    spell = SpellChecker()\n",
    "    temp_spellchecked = []\n",
    "    for word in temp.split():\n",
    "        temp_spellchecked.append(spell.correction(word))\n",
    "        \n",
    "    # stemming with nltk\n",
    "    #stemmer = SnowballStemmer(language='english')\n",
    "    #temp_stemmed = []\n",
    "    #for word in temp_spellchecked:\n",
    "    #    temp_stemmed.append(stemmer.stem((word)))\n",
    "    \n",
    "    # create spacy object\n",
    "    temp = sp(' '.join(temp_spellchecked))\n",
    "\n",
    "    # lemmatize each token and convert each token into lowercase\n",
    "    temp = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in temp ]\n",
    "    \n",
    "    # remove stop words \n",
    "    temp = [ word for word in temp if word not in stop_words  ]\n",
    "    \n",
    "    # join\n",
    "    temp = ' '.join(temp)\n",
    "    \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# clean text\n",
    "train.text = train.text.apply(lambda x: clean_text(x))\n",
    "test.text = test.text.apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train_spellchecked.csv')\n",
    "test.to_csv('test_spellchecked.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train_spellchecked.csv')\n",
    "test = pd.read_csv('../data/test_spellchecked.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.text.apply(lambda x: print(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.location.isnull().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ† [D.] 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pycountry.countries.search_fuzzy('England')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# not good \n",
    "def location_to_country(location):\n",
    "    temp = location.split(',')\n",
    "    countries = list(pycountry.countries)\n",
    "    \n",
    "    for word in temp:\n",
    "        for i in range(len(countries)):\n",
    "            if (word.strip() in countries[i].alpha_2) or (word.strip() in countries[i].alpha_3) or (word.strip() in countries[i].name):\n",
    "                return countries[i].name\n",
    "    return 'Unknown'\n",
    "\n",
    "train['country'] = train.location.apply(lambda x: location_to_country(str(x)))\n",
    "train[['location', 'country']].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è 6. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer function for preprocessing\n",
    "def spacy_tokenizer(text):\n",
    "\n",
    "    # Define stopwords, punctuation, rolex and numbers\n",
    "    #stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "    #punctuations = string.punctuation\n",
    "    # numbers = \"0123456789\"\n",
    "\n",
    "    # Create spacy object\n",
    "    mytokens = sp(text)\n",
    "\n",
    "    #Lemmatize each token and convert each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "\n",
    "    # Remove stop words and punctuation\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "    #\n",
    "    # Remove all word with less that 3 letters (remove noise)\n",
    "    mytokens = [ word for word in mytokens if len(word)>2 ]\n",
    "\n",
    "    # Return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize texts\n",
    "processed_texts = []\n",
    "for text in train.text:\n",
    "    processed_text = spacy_tokenizer(text)\n",
    "    processed_texts.append(processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ 7. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/training_data_spellchecked.csv')\n",
    "test = pd.read_csv('../data/test_data_spellchecked.csv')\n",
    "sample_submission = pd.read_csv('../data/sample_submission.csv')\n",
    "\n",
    "# change type to string to prevent some errors\n",
    "train.text = train.text.astype(str)\n",
    "train.keyword = train.keyword.astype(str)\n",
    "train.location = train.location.astype(str)\n",
    "\n",
    "test.text = test.text.astype(str)\n",
    "test.keyword = test.keyword.astype(str)\n",
    "test.location = test.location.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll clean this part during the week - Lo√Øc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using default tokenizer \n",
    "count = CountVectorizer(ngram_range=(1,2), stop_words=\"english\")\n",
    "bow = count.fit_transform(train.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names\n",
    "feature_names = count.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show as a dataframe\n",
    "processed_train = pd.DataFrame(\n",
    "    bow.todense(), \n",
    "    columns=feature_names\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features\n",
    "X = processed_train # the features we want to analyze\n",
    "y = train['target'] # the labels, or answers, we want to test against\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classifier\n",
    "classifier = LogisticRegressionCV(solver='lbfgs', max_iter=1000, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model on training set\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "print(round(accuracy_score(y_test, y_pred), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOW with more additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full = pd.concat([train[['num_char', 'num_words', 'avg_word_length', 'num_hashtags']], processed_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features\n",
    "X = processed_train # the features we want to analyze\n",
    "y = train['target'] # the labels, or answers, we want to test against\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classifier\n",
    "classifier = LogisticRegressionCV(solver='lbfgs', max_iter=3000, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit model on training set\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "print(round(accuracy_score(y_test, y_pred), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOW with additional features and Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full = pd.concat([train[['num_char', 'num_words', 'avg_word_length']], processed_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features\n",
    "X = processed_train # the features we want to analyze\n",
    "y = train['target'] # the labels, or answers, we want to test against\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classifier\n",
    "classifier = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model on training set\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "print(round(accuracy_score(y_test, y_pred), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features\n",
    "X = train['text'] # the features we want to analyze\n",
    "y = train['target'] # the labels, or answers, we want to test against\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=707)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Define vectorizer\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 1), tokenizer=spacy_tokenizer)\n",
    "\n",
    "# Define classifier\n",
    "classifier = LogisticRegressionCV(solver='lbfgs', max_iter=1000, cv=5)\n",
    "\n",
    "# Create pipeline\n",
    "pipe = Pipeline([('vectorizer', tfidf),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "# Fit model on training set\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "print(round(accuracy_score(y_test, y_pred), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perhaps a random forest? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe try a Random Forest? (- Hanna)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define vectorizer\n",
    "tfidf_vector = TfidfVectorizer(tokenizer=spacy_tokenizer) \n",
    "\n",
    "# Define classifier\n",
    "classifier = RandomForestClassifier()\n",
    "\n",
    "# Create pipeline\n",
    "pipe = Pipeline([('vectorizer', tfidf_vector),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "# Fit model on training set\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Define vectorizer\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 2), tokenizer=spacy_tokenizer)\n",
    "\n",
    "# Define classifier\n",
    "classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Create pipeline\n",
    "pipe = Pipeline([('vectorizer', tfidf),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "# Fit model on training set\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "print(round(accuracy_score(y_test, y_pred), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using Doc2Vec and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tagged = train.apply(lambda r: TaggedDocument(words=spacy_tokenizer(r['text']), tags=[r.target]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split - same split as before\n",
    "train_tagged, test_tagged = train_test_split(sample_tagged, test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows to speed up a bit\n",
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Doc2Vec and build vocabulary\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "model_dbow = Doc2Vec(dm=1, vector_size=100, negative=5, hs=0, min_count=1, sample=0, workers=cores, epoch=500)\n",
    "model_dbow.build_vocab([x for x in train_tagged.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train distributed Bag of Word model\n",
    "model_dbow.train(train_tagged, total_examples=model_dbow.corpus_count, epochs=model_dbow.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select X and y\n",
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=300)) for doc in sents])\n",
    "    return targets, regressors\n",
    "\n",
    "y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dbow, test_tagged)\n",
    "\n",
    "# Each document (i.e. complaint) is now a vector in the space of 30 dimentions.\n",
    "# Similar complaints should have similar vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(cv=10, max_iter=3000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model on training set - same algorithm as before\n",
    "logreg = LogisticRegressionCV(max_iter=3000, cv=10, solver='lbfgs')\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7884\n",
      "cv = 9, basique, clean√© + pays 0.7853\n",
      "0.7869\n",
      "avec cv = 9 :  0.7892\n",
      "avec cv = 9, basique clean:  0.7923\n",
      "avec cv = 9, + features  0.7861\n",
      "avec cv = 9, basique, pas clean [import, token --> c tout] 0.7946\n",
      "avec cv = 10, basique, pas clean [import, token --> c tout] 0.7954\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "print(round(accuracy_score(y_test, y_pred), 4))\n",
    "print('cv = 9, basique, clean√© + pays',0.7853)\n",
    "print(0.7869)\n",
    "print('avec cv = 9 : ', 0.7892)\n",
    "print('avec cv = 9, basique clean: ', 0.7923)\n",
    "print('avec cv = 9, + features ', 0.7861)\n",
    "print('avec cv = 9, basique, pas clean [import, token --> c tout]', 0.7946)\n",
    "print('avec cv = 10, basique, pas clean [import, token --> c tout]', 0.7954)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using Doc2Vec, more features and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'glove.6B.300d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-be52a9192ac4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtotal_examples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"glove.6B.300d.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mmodel_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python 3.x\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m   1545\u001b[0m         \"\"\"\n\u001b[0;32m   1546\u001b[0m         \u001b[1;31m# from gensim.models.word2vec import load_word2vec_format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1547\u001b[1;33m         return _load_word2vec_format(\n\u001b[0m\u001b[0;32m   1548\u001b[0m             \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1549\u001b[0m             limit=limit, datatype=datatype)\n",
      "\u001b[1;32md:\\python 3.x\\lib\\site-packages\\gensim\\models\\utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, binary_chunk_size)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loading projection weights from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 275\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    276\u001b[0m         \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# throws for invalid file format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python 3.x\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[0mtransport_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m     fobj = _shortcut_open(\n\u001b[0m\u001b[0;32m    181\u001b[0m         \u001b[0muri\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python 3.x\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[1;34m(uri, mode, ignore_ext, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'errors'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove.6B.300d.txt'"
     ]
    }
   ],
   "source": [
    "model_2 = Word2Vec(size=300, min_count=1)\n",
    "model_2.build_vocab(train.text)\n",
    "total_examples = model_2.corpus_count\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(\"glove.6B.300d.txt\", binary=True)\n",
    "\n",
    "model_2.build_vocab([list(model.vocab.keys())], update=True)\n",
    "\n",
    "model_2.intersect_word2vec_format(\"glove.6B.300d.txt\", binary=True, lockf=1.0)\n",
    "\n",
    "model_2.train(train.text, total_examples=total_examples, epochs=model_2.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-8c327cf8ff64>:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  X = model_1[model_1.wv.vocab]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'glove.6B.300d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-8c327cf8ff64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mmodel_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mtotal_examples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"glove.6B.300d.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[0mmodel_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mmodel_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintersect_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"glove.6B.300d.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlockf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python 3.x\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m   1545\u001b[0m         \"\"\"\n\u001b[0;32m   1546\u001b[0m         \u001b[1;31m# from gensim.models.word2vec import load_word2vec_format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1547\u001b[1;33m         return _load_word2vec_format(\n\u001b[0m\u001b[0;32m   1548\u001b[0m             \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1549\u001b[0m             limit=limit, datatype=datatype)\n",
      "\u001b[1;32md:\\python 3.x\\lib\\site-packages\\gensim\\models\\utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, binary_chunk_size)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loading projection weights from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 275\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    276\u001b[0m         \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# throws for invalid file format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python 3.x\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[0mtransport_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m     fobj = _shortcut_open(\n\u001b[0m\u001b[0;32m    181\u001b[0m         \u001b[0muri\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python 3.x\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[1;34m(uri, mode, ignore_ext, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'errors'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove.6B.300d.txt'"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "# define training data\n",
    "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "            ['this', 'is', 'the', 'second', 'sentence'],\n",
    "            ['yet', 'another', 'sentence'],\n",
    "            ['one', 'more', 'sentence'],\n",
    "            ['and', 'the', 'final', 'sentence']]\n",
    "# train model\n",
    "model_1 = Word2Vec(sentences, size=300, min_count=1)\n",
    "\n",
    "# fit a 2d PCA model to the vectors\n",
    "X = model_1[model_1.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "# create a scatter plot of the projection\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "words = list(model_1.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model_2 = Word2Vec(size=300, min_count=1)\n",
    "model_2.build_vocab(sentences)\n",
    "total_examples = model_2.corpus_count\n",
    "model = KeyedVectors.load_word2vec_format(\"glove.6B.300d.txt\", binary=False)\n",
    "model_2.build_vocab([list(model.vocab.keys())], update=True)\n",
    "model_2.intersect_word2vec_format(\"glove.6B.300d.txt\", binary=False, lockf=1.0)\n",
    "model_2.train(sentences, total_examples=total_examples, epochs=model_2.iter)\n",
    "\n",
    "# fit a 2d PCA model to the vectors\n",
    "X = model_2[model_1.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "# create a scatter plot of the projection\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "words = list(model_1.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèÜ 8. Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using default tokenizer \n",
    "count = CountVectorizer(ngram_range=(1,2), stop_words=\"english\")\n",
    "bow = count.fit(train.text)\n",
    "bow = count.transform(train.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names\n",
    "feature_names = count.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show as a dataframe\n",
    "processed_train = pd.DataFrame(\n",
    "    bow.todense(), \n",
    "    columns=feature_names\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full = pd.concat([train[['num_char', 'num_words', 'avg_word_length', 'num_hashtags']], processed_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features\n",
    "X = train_full # the features we want to analyze\n",
    "y = train['target'] # the labels, or answers, we want to test against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classifier\n",
    "classifier = LogisticRegressionCV(solver='lbfgs', max_iter=6000, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit model on training set\n",
    "classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_test = count.transform(test.text)\n",
    "# Get feature names\n",
    "feature_names_test = count.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show as a dataframe\n",
    "processed_test = pd.DataFrame(\n",
    "    bow_test.todense(),\n",
    "    columns=feature_names_test\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_full = pd.concat([test[['num_char', 'num_words', 'avg_word_length' , 'num_hashtags']], processed_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "\n",
    "y_pred = classifier.predict(test_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline\n",
    "pipe = Pipeline([('vectorizer', tfidf),\n",
    "                 ('classifier', classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(train.text, train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pipe.predict(test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/training_data_spellchecked.csv')\n",
    "test = pd.read_csv('../data/test_data_spellchecked.csv')\n",
    "\n",
    "train[['location', 'text']] = train[['location', 'text']].astype(str)\n",
    "test['target'] = ''\n",
    "test[['location', 'text']] = test[['location', 'text']].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tagged = train.apply(lambda r: TaggedDocument(words=spacy_tokenizer(r['text']), tags=[r.target]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tagged = test.apply(lambda r: TaggedDocument(words=spacy_tokenizer(r['text']), tags=[r.target]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows to speed up a bit\n",
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Doc2Vec and build vocabulary\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "model_dbow = Doc2Vec(dm=0, vector_size=30, negative=6, hs=0, min_count=1, sample=0, workers=cores, epoch=300)\n",
    "model_dbow.build_vocab([x for x in train_tagged.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train distributed Bag of Word model\n",
    "model_dbow.train(train_tagged, total_examples=model_dbow.corpus_count, epochs=model_dbow.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select X and y\n",
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=300)) for doc in sents])\n",
    "    return targets, regressors\n",
    "\n",
    "y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dbow, test_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegressionCV(max_iter=1000, solver='lbfgs', cv=3)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.target = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv('submission-005.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
